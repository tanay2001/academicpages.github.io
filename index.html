
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name=viewport content="width=800">
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    
    a {
      color: #1772d0;
      text-decoration: none;
    }
    
    a:focus,
    a:hover {
      color: #f09228;
      text-decoration: none;
    }
    
    body,
    td,
    th,
    tr,
    p,
    a {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px
    }
    
    strong {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
    }
    
    heading {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 22px;
    }
    
    papertitle {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 15px;
      font-weight: 700
    }
    
    name {
      font-family: 'Lato', Verdana, Helvetica, sans-serif;
      font-size: 32px;
    }
    
    .one {
      width: 160px;
      height: 120px;
      position: relative;
    }

    
    span.highlight {
      background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="images/profile_pic.jpeg">
  <title>Tanay Dixit</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-167314770-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-167314770-1');
  </script>
</head>

<body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
      <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="67%" valign="middle">
              <p align="center">
                <name>Tanay Dixit</name>
              </p>
              <p>I am an incoming MSCS student at the University of Illinois, Urbana Champaign, where I would be working with <a href="http://dm1.cs.uiuc.edu/" target="_blank">Prof. Jiawei Han</a>. Previously, I interned at Microsoft Research, India wherein I had the pleasure of working with <a href="https://www.microsoft.com/en-us/research/people/sriram/" target="_blank">Sriram Rajamani</a> and <a href="https://www.microsoft.com/en-us/research/people/pratykumar/" target="_blank">Pratyush Kumar</a> on leveraging Large Language Models for code generation.
              </p>
              <p> I graduated from Indian Institute of Technology Madras in 2023 with a bachelors degree in Electrical Engineering and a minor in Computing. While at IIT Madras, I worked with <a href="https://www.cse.iitm.ac.in/~miteshk/" target="_blank">Prof. Mitesh Khapra</a> on analyzing and developing resources to evaluate the effectiveness of text generation evaluation metrics. I interned at the <a href="https://h2lab.cs.washington.edu/" target="_blank">H2Lab, University of Washington</a> where I worked with <a href="https://homes.cs.washington.edu/~hannaneh/index.html" target="_blank">Prof. Hanna Hajishirzi</a> on developing counterfactual data generation techniques for model interpretability. I also interned at University of Southern California, where I worked with <a href="https://muhaochen.github.io/" target="_blank">Prof. Muhao Chen</a> on improving factuality of summarization systems.
              </p>
              <p>
              I'm currently looking for internships for Summer 2024 in the fields of machine learning and natural language processing.
              </p>
              
              <p align=center>
                <a href="mailto:tanayd2@illinois.edu" target="_blank">Email</a> &nbsp/&nbsp
                <a href="data/CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/tanaydixit/" target="_blank">LinkedIn</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?hl=en&user=21JBGiUAAAAJ" target="_blank">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.semanticscholar.org/author/Tanay-Dixit/2126503480" target="_blank">Semantic Scholar</a>                 
              </p>
            </td>
            <td width="33%">
              <img style="width:100%;max-width:100%" src="images/profile_pic.png">
            </td>
          </tr>
        </table>
        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Research</heading>
              <p>
                I'm interested in natural language processing, particulary in the fields of interpretability and trustworthiness.
              </p>
            </td>
          </tr>
          
          
        </table> -->

        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Preprints</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          

          
        </table> -->

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td width="100%" valign="middle">
              <heading>Publications</heading>
            </td>
          </tr>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

          <tr>
            <td width="25%">
              <div class="one">
                  <a href="images/efactsum.png" target="_blank"><img style="width:100%;max-width:100%" src='images/efactsum.png'></a>
              </div>
              
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2305.14981" target="_blank">
                  <papertitle>Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality</papertitle>
              </a>
              <br>
              <strong>Tanay Dixit</strong>, Fei Wang, Muhao Chen
              <br>
              <em>ACL 2023</em>
              <br><br>
              [<a href="https://arxiv.org/pdf/2305.14981" target="_blank">Paper</a>][<a href="https://github.com/tanay2001/
              EFactSum" target="_blank">Code</a>]
              <br><br><br>
              <!-- <p>Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose EFACTSUM (i.e., Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing summary quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.</p> -->
            </td>
          </tr>  

          <tr>
            <td width="25%">
              <div class="one">
                  <a href="images/IndicMT.png" target="_blank"><img style="width:100%;max-width:100%" src='images/IndicMT.png'></a>
              </div>
              
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2212.10180" target="_blank">
                  <papertitle>IndicMT Eval: A Dataset to Meta-Evaluate Machine Translation Metrics for Indian Languages</papertitle>
              </a>
              <br>
              Ananya Sai, <strong>Tanay Dixit</strong>, Vignesh Nagarajan, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra, Raj Dabre
              <br>
              <em>ACL 2023</em>
              <br>
              [<a href="https://arxiv.org/pdf/2212.10180" target="_blank">Paper</a>][<a href="https://github.com/AI4Bharat/IndicMT-Eval" target="_blank">Code</a>]
              <br><br><br>
              <!-- <p>The rapid growth of machine translation (MT) systems has necessitated comprehensive studies to meta-evaluate evaluation metrics being used, which enables a better selection of metrics that best reflect MT quality. Unfortunately, most of the research focuses on high-resource languages, mainly English, the observations for which may not always apply to other languages. Indian languages, having over a billion speakers, are linguistically different from English, and to date, there has not been a systematic study of evaluating MT systems from English into Indian languages. In this paper, we fill this gap by creating an MQM dataset consisting of 7000 fine-grained annotations, spanning 5 Indian languages and 7 MT systems, and use it to establish correlations between annotator scores and scores obtained using existing automatic metrics. Our results show that pre-trained metrics, such as COMET, have the highest correlations with annotator scores. Additionally, we find that the metrics do not adequately capture fluency-based errors in Indian languages, and there is a need to develop metrics focused on Indian languages. We hope that our dataset and analysis will help promote further research in this area.</p> -->
              </td>
          </tr>          

          <tr>
            <td width="25%">
              <div class="one">
                  <a href="images/CORE.png" target="_blank"><img style="width:100%;max-width:100%" src='images/CORE.png'></a>
              </div>
              
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2210.04873" target="_blank">
                  <papertitle>CORE: A Retrieve-then-Edit Framework for Counterfactual Data Generation</papertitle>
              </a>
              <br>
              <strong>Tanay Dixit</strong>,Bhargavi Paranjape, Hannaneh Hajishirzi, Luke Zettlemoyer
              <br>
              <em>EMNLP 2022 (Findings)</em>
              <br>
              [<a href="https://arxiv.org/pdf/2210.04873" target="_blank">Paper</a>][<a href="https://github.com/tanay2001/CORE" target="_blank">Code</a>]
              <br><br><br>
              <!-- <p>Counterfactual data augmentation (CDA) -- i.e., adding minimally perturbed inputs during training -- helps reduce model reliance on spurious correlations and improves generalization to out-of-distribution (OOD) data. Prior work on generating counterfactuals only considered restricted classes of perturbations, limiting their effectiveness. We present COunterfactual Generation via Retrieval and Editing (CORE), a retrieval-augmented generation framework for creating diverse counterfactual perturbations for CDA. For each training example, CORE first performs a dense retrieval over a task-related unlabeled text corpus using a learned bi-encoder and extracts relevant counterfactual excerpts. CORE then incorporates these into prompts to a large language model with few-shot learning capabilities, for counterfactual editing. Conditioning language model edits on naturally occurring data results in diverse perturbations. Experiments on natural language inference and sentiment analysis benchmarks show that CORE counterfactuals are more effective at improving generalization to OOD data compared to other DA approaches. We also show that the CORE retrieval framework can be used to encourage diversity in manually authored perturbations</p> -->
              </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                  <a href="images/Instructv2.png" target="_blank"><img style="width:100%;max-width:100%" src='images/Instructv2.png'></a>
              </div>
              
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2204.07705" target="_blank">
                  <papertitle>SUPER-NATURALINSTRUCTIONS: Generalization via Declarative Instructions on 1600+ NLP Tasks</papertitle>
              </a>
              <br>
              Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, ... , <strong>Tanay Dixit</strong>, ... , Hannaneh Hajishirzi, Daniel Khashabi
              <br>
              <em>EMNLP, 2022</em>
              <br>
              [<a href="https://arxiv.org/pdf/2204.07705.pdf" target="_blank">Paper</a>][<a href="https://instructions.apps.allenai.org/" target="_blank">Code</a>]
              <br><br><br><br><br>
              <!-- <p>How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.</p> -->
              </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                  <a href="images/checklist.png" target="_blank"><img style="width:100%;max-width:100%" src='images/checklist.png'></a>
              </div>
              
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2109.05771.pdf" target="_blank">
                  <papertitle>Perturbation CheckLists for Evaluating NLG Evaluation Metrics</papertitle>
              </a>
              <br>
              Ananya B Sai, <strong>Tanay Dixit</strong>, Dev Yashpal Sheth, Sreyas Mohan, Mitesh M Khapra
              <br>
              <em>EMNLP, 2021</em>
              <br>
              [<a href="https://arxiv.org/pdf/2109.05771.pdf" target="_blank">Paper</a>][<a href="https://iitmnlp.github.io/EvalEval" target="_blank">Code</a>]
              <br><br><br>
              <!-- <p>Natural Language Generation (NLG) evaluation is a multifaceted task requiring assessment of multiple desirable criteria, e.g., fluency, coherency, coverage, relevance, adequacy, overall quality, etc. Across existing datasets for 6 NLG tasks, we observe that the human evaluation scores on these multiple criteria are often not correlated. For example, there is a very low correlation between human scores on fluency and data coverage for the task of structured data to text generation. This suggests that the current recipe of proposing new automatic evaluation metrics for NLG by showing that they correlate well with scores assigned by humans for a single criteria (overall quality) alone is inadequate. Indeed, our extensive study involving 25 automatic evaluation metrics across 6 different tasks and 18 different evaluation criteria shows that there is no single metric which correlates well with human scores on all desirable criteria, for most NLG tasks. Given this situation, we propose CheckLists for better design and evaluation of automatic metrics. We design templates which target a specific criteria (e.g., coverage) and perturb the output such that the quality gets affected only along this specific criteria (e.g.,the coverage drops). We show that existing evaluation metrics are not robust against even such simple perturbations and disagree with scores assigned by humans to the perturbed output. The proposed templates thus allow for a fine-grained assessment of automatic evaluation metrics exposing their limitations and will facilitate better design, analysis and evaluation of such metrics</p> -->
              </td>
          </tr>

          <tr>
            <td width="25%">
              <div class="one">
                  <a href="images/nlaug.png" target="_blank"><img style="width:100%;max-width:100%" src='images/nlaug.png'></a>
              </div>
              
            </td>
            <td valign="top" width="75%">
              <a href="https://arxiv.org/pdf/2112.02721.pdf">
                  <papertitle>NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation</papertitle>
              </a>
              <br>
                Kaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, ... , <strong>Tanay Dixit</strong>, ... (many authors)
              <br>
              <em>NEJLT 2023 (GEM Workshop, IJCNLP 2021) </em>
              <br>
              [<a href="https://arxiv.org/pdf/2112.02721.pdf" target="_blank">Paper</a>][<a href="https://github.com/GEM-benchmark/NL-Augmenter" target="_blank">Code</a>]
              <br><br><br>
              <!-- <p>Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Pythonbased natural language augmentation framework which supports the creation of both transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of natural language tasks. We demonstrate the efficacy of NL-Augmenter by using several of its tranformations to analyze the robustness of popular natural language models.</p> -->
              </td>
          </tr>


        
        
		   
        
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr>
            <td>
              <br>
              <p align="right">
                <font size="2">
                  <br><a href="https://jonbarron.info/">Template from here</a></font>
              </p>
            </td>
          </tr>
        </table>
        

        </td>
    </tr>
  </table>
</body>

</html>